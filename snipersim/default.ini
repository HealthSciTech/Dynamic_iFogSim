################
# General XCSF #
################

OMP_NUM_THREADS=8 # number of threads for parallel processing
POP_SIZE=2000 # maximum number of macro-classifiers in the population
MAX_TRIALS=2000 # number of learning trials to perform
POP_INIT=true # whether to fill the initial population with random classifiers
PERF_TRIALS=500 # number of trials to average performance output
LOSS_FUNC=0 # Mean Absolute Error loss function (use for mazes and mux)
#LOSS_FUNC=1 # Mean Squared Error
#LOSS_FUNC=2 # Root Mean Squared Error
#LOSS_FUNC=3 # Log loss; for multi-class classification
#LOSS_FUNC=4 # Binary log loss; for binary-class classification
#LOSS_FUNC=5 # One-hot encoding accuracy; for classification

#########################
# Multi-step parameters #
#########################

TELETRANSPORTATION=50 # num steps to reset a multistep problem if goal not found
GAMMA=0.95 # discount factor in calculating the reward for multistep problems
P_EXPLORE=0.9 # probability of exploring vs. exploiting in a multistep trial

######################
# General Classifier #
######################

EPS_0=0.01 # classifier target error, under which accuracy is set to 1
ALPHA=0.1 # classifier accuracy offset for rules above EPS_0 (1=disabled)
NU=5.0 # classifier accuracy slope for rules with error above EPS_0
BETA=0.1 # learning rate for updating error, fitness, and set size
DELTA=0.1 # fraction of least fit classifiers to increase deletion vote
THETA_DEL=20 # min experience before fitness used in probability of deletion
INIT_FITNESS=0.01 # initial classifier fitness
INIT_ERROR=0.0 # initial classifier error
ERR_REDUC=1.0 # amount to reduce an offspring's error (1=disabled)
FIT_REDUC=0.1 # amount to reduce an offspring's fitness (1=disabled)
M_PROBATION=10000 # trials since creation a rule must match at least 1 input or be deleted

###############
# Subsumption #
###############

EA_SUBSUMPTION=false # whether to try and subsume offspring classifiers
SET_SUBSUMPTION=false # whether to perform match set subsumption
THETA_SUB=1000 # minimum experience of a classifier to become a subsumer
 
##########################
# Evolutionary Algorithm #
##########################

EA_SELECT_TYPE=0 # roulette wheel parental selection
#EA_SELECT_TYPE=1 # tournament parental selection
EA_SELECT_SIZE=0.4 # fraction of set size for tournament parental selection
THETA_EA=50.0 # average set time between EA invocations
LAMBDA=2 # number of offspring to create each EA invocation
P_CROSSOVER=0.8 # probability of applying crossover
SAM_TYPE=0 # log normal self-adaptive mutation
#SAM_TYPE=1 # ten normally distributed self-adaptive mutation rates
 
########################
# Classifier Condition #
########################

COND_ETA=0.0 # gradient descent rate for updating the condition

# Condition type
#COND_TYPE=0 # always matching dummy condition
COND_TYPE=1 # hyperrectangles
#COND_TYPE=2 # hyperellipsoids
#COND_TYPE=3 # multi-layer perceptron neural networks
#COND_TYPE=4 # GP trees
#COND_TYPE=5 # dynamical GP graphs
#COND_TYPE=6 # ternary (binarises inputs)
#COND_TYPE=11 # both conditions and actions in single dynamical GP graphs
#COND_TYPE=12 # both conditions and actions in single neural networks

# Hyperrectangles and hyperellipsoids
COND_MIN=0.0 # minimum input value
COND_MAX=1.0 # maximum input value
COND_SMIN=0.1 # minimum initial spread

# Ternary
COND_BITS=1 # bits per float to binarise inputs for ternary conditions (mp=1, maze=2 or 3)

# Tree-GP
GP_NUM_CONS=100 # number of (shared) constants available for GP trees 
GP_INIT_DEPTH=5 # initial depth of GP trees

# DGP
DGP_NUM_NODES=20 # number of nodes in a DGP graph
RESET_STATES=false # whether to reset DGP initial states each trial
MAX_K=2 # maximum number of connections a DGP node may have
MAX_T=10 # maximum number of cycles to update a DGP graph

# Neural network
MAX_NEURON_MOD=1 # maximum number of neurons to add or remove during mutation

COND_EVOLVE_WEIGHTS=true # whether to evolve the weights
COND_EVOLVE_NEURONS=true # whether to evolve the number of neurons
COND_EVOLVE_FUNCTIONS=false # whether to evolve the activation functions
COND_NUM_NEURONS=1 # initial number of hidden neurons in each layer
COND_MAX_NEURONS=10 # maximum number of hidden neurons in each layer (if evolved)
# Neural Network layer activation functions (if not evolved)
COND_OUTPUT_ACTIVATION=0 # Logistic [0,1]
COND_HIDDEN_ACTIVATION=0 # Logistic [0,1]
# ACTIVATION=1 # Rectified linear unit [0,inf]
# ACTIVATION=2 # TanH [-1,1]
# ACTIVATION=3 # Linear [-inf,inf]
# ACTIVATION=4 # Gaussian (0,1]
# ACTIVATION=5 # Sinusoid [-1,1]
# ACTIVATION=6 # Cosine [-1,1]
# ACTIVATION=7 # Soft plus [0,inf]
# ACTIVATION=8 # Leaky rectified linear unit [-inf,inf]
# ACTIVATION=9 # Scaled exponential linear unit [-1.7581,inf]
# ACTIVATION=10 # Logistic [-1,1]

#########################
# Classifier Prediction #
#########################

# Prediction type
#PRED_TYPE=0 # constant
PRED_TYPE=1 # linear least squares
#PRED_TYPE=2 # quadratic least squares
#PRED_TYPE=3 # linear recursive least squares
#PRED_TYPE=4 # quadratic recursive least squares
#PRED_TYPE=5 # stochastic gradient descent multilayer perceptron neural networks

PRED_EVOLVE_ETA=true # whether to use self-adaptive mutation to set eta [0.0001,0.1]
PRED_ETA=0.1 # fixed gradient descent rate for updating the prediction
PRED_RESET=false # whether to reset offspring predictions instead of copying

# Least squares
PRED_X0=1.0 # prediction weight vector offset value
PRED_RLS_SCALE_FACTOR=1000.0 # initial diagonal values of the RLS gain-matrix
PRED_RLS_LAMBDA=1.0 # forget rate for RLS (small values may be unstable)

# Neural network
PRED_EVOLVE_WEIGHTS=true # whether to evolve the weights
PRED_EVOLVE_NEURONS=true # whether to evolve the number of neurons
PRED_EVOLVE_FUNCTIONS=false # whether to evolve the activation functions
PRED_SGD_WEIGHTS=true # whether to apply stochastic gradient descent
PRED_MOMENTUM=0.9 # momentum for gradient descent update
PRED_NUM_NEURONS=1,1 # initial number of hidden neurons in each layer
PRED_MAX_NEURONS=10,10 # maximum number of hidden neurons in each layer (if evolved)
# Neural Network layer activation functions (if not evolved)
PRED_OUTPUT_ACTIVATION=0 # Logistic [0,1]
PRED_HIDDEN_ACTIVATION=0 # Logistic [0,1]
# ACTIVATION=1 # Rectified linear unit [0,inf]
# ACTIVATION=2 # TanH [-1,1]
# ACTIVATION=3 # Linear [-inf,inf]
# ACTIVATION=4 # Gaussian (0,1]
# ACTIVATION=5 # Sinusoid [-1,1]
# ACTIVATION=6 # Cosine [-1,1]
# ACTIVATION=7 # Soft plus [0,inf]
# ACTIVATION=8 # Leaky rectified linear unit [-inf,inf]
# ACTIVATION=9 # Scaled exponential linear unit [-1.7581,inf]
# ACTIVATION=10 # Logistic [-1,1]

#####################
# Classifier Action #
#####################

# Action type
ACT_TYPE=0 # integer
